# This code is used to generate an analysis html for the results of the mturk batch of project -
# Explainable KBC (id=1365457).
# It requires the html book file (generated by @get_turk_data.py), and the results.csv downloaded from mturk.
# It then generates an analysis html file if all the HITs are valid, if Not it generates a CSV with a reason for rejecting the HIT.
# Upload that CSV to Mturk to reject the HITs, not pay the turkers and republish the hits for other workers to do.
import pandas as pd
import numpy as np
import pprint
import argparse
import collections
import string
import os
import bs4 as bs
import itertools
from collections import Counter
import csv
ANSWER_OPTIONS = ['our','other','both','none']

def get_key_answer(key,id):
    return string.Template('Answer.${key}_${id}.on').substitute(key=key,id=id)

def get_key_input(key,id):
    return string.Template('Input.${key}_${id}').substitute(key=key,id=id)

def valid_row(row):
    total_sum = 0
    quality_ctrl_id = None
    for i in range(5):
        if(row[get_key_input('exp_A',i)] == row[get_key_input('exp_B',i)]):
            quality_ctrl_id = i
        for opt in ANSWER_OPTIONS:
            total_sum += row[get_key_answer(opt,i)]
    if(total_sum != 5):
        return 'You did not mark any option in some questions'
    if(quality_ctrl_id is not None):
        if(not (row[get_key_answer('both',quality_ctrl_id)] or row[get_key_answer('none',quality_ctrl_id)]) ):
            print("Quality control id == >",quality_ctrl_id)
            return 'You did not chose the option both explanations are good/bad, even when both A and B were same in question number {}'.format(quality_ctrl_id+1)
    return ''

def get_invalid_hits(df,outfilename):
    df_new = df.copy()
    df = df.fillna(False)
    invalid_hits = collections.defaultdict(list)
    for index,row in df.iterrows():
        message = valid_row(row)
        if(message!=''):
            print('Invalid HIT at {} with message ==> {} '.format(index, message))
            df_new['Reject'][index] = message
            invalid_hits[row['WorkerId']].append(row['AssignmentId'])
        #else:
        #    df_new['Approve'][index] = 'X'
    if(len(invalid_hits)!=0):
        df_new.to_csv(outfilename,index=False,sep=',',quoting=csv.QUOTE_ALL)
    return invalid_hits

def get_book(book_filename):
    # TODO: Change this to have a clean pipeline
    with open(book_filename,'r') as f:
        soup = bs.BeautifulSoup(f, 'lxml')
        table = soup.find('table')
        table_body = table.find('tbody')
        rows = table_body.find_all('tr')
        data = []
        for row in rows:
            cols = row.find_all('td')
            cols = [ele.text for ele in cols]
            data.append([ele for ele in cols if ele])
    return pd.DataFrame(data,columns=['fact','our','other','qid'])




def get_winner_majority(answers):
    count_choices = Counter(answers)
    ordered_choices = count_choices.most_common()
    first =  ordered_choices[0][0]
    first_count = ordered_choices[0][1]
    second = first 
    second_count = -1
    if len(ordered_choices) > 1:
        second = ordered_choices[1][0]
        second_count = ordered_choices[1][1]
   
    #if (first_count == second_count):
    #    if set([first,second]) == set(['our','other']):
    #        first = 'tie'

    total = sum(count_choices.values())
    repeat = total < args.min_count
    if repeat: 
        print('Repeat because total: {} < min_count: {}'.format(total,args.min_count))

    if first in ['both','none']:
        first = 'tie'
    #

    if 1.0*first_count/total < args.threshold:
        repeat = True
        first = 'undec'
        if repeat:
            print('Repeat because {}/{} < thr: {}'.format(ordered_choices[0][1], total, args.threshold))
    #
    return [first], repeat


def get_winner(answers):
    if args.selection_type == 'voting':
        return get_winner_voting(answers)
    elif args.selection_type == 'majority':
        return get_winner_majority(answers)
    else:
        raise

def get_winner_voting(answers):
    our_ct = 0
    other_ct = 0
    for el in answers:
        if(el=='our'):
            our_ct += 1
        elif(el=='other'):
            other_ct += 1
        elif(el=='both'):
            our_ct += 1
            other_ct += 1

    if(our_ct > other_ct):
        return ['our'], False 
    elif (other_ct > our_ct):
        return ['other'], False 
    else:
        return ['tie'], False 
        # return ['our','other']

def get_results(dflist,book):
    results = {}
    to_repeat_qids = []
    for df in dflist:
        df = df.fillna(False)
        for index, row in df.iterrows():
            for i in range(5):
                fact = row[get_key_input('fact',i)]
                exp_A = row[get_key_input('exp_A',i)]
                exp_B = row[get_key_input('exp_B',i)]
                qid = int(row.get(get_key_input('qid',i),-1))
                explanation = row.get('Answer.explanation'+str(i),'NO EXP')
                if(exp_A == exp_B):
                    continue

                fact_text = bs.BeautifulSoup(fact,'lxml').text
                exp_B_text = bs.BeautifulSoup(exp_B,'lxml').text
                our_expln = 'A'
                if(book[book.fact == fact_text]['our'].iloc[0] == exp_B_text):
                    exp_A, exp_B = exp_B , exp_A
                    our_expln = 'B' 
                #
                qid_from_book = int(book.qid[book.fact == fact_text].iloc[0])
                if qid != -1:
                    print(qid,qid_from_book)
                    # assert qid == qid_from_book
                if(fact not in results):
                    results[fact] = {'our_exp': exp_A,'other_exp':exp_B, 'answers' : [],'row_idx':[], 'fact_no':[],'qid': qid_from_book,'explanation':[],'ours_is': []}
                if qid != -1 and results[fact]['qid'] != -1:
                    pass
                    # assert results[fact]['qid'] == qid
                else:
                    #for the case when qid is present only in one file. we retain the bigger qid , i.e. the one which is not -1
                    results[fact]['qid'] = max(qid,results[fact]['qid'])
                #
                #assert our_expln == results[fact]['ours_is']
                results[fact]['explanation'].append(explanation)
                for opt in ANSWER_OPTIONS:
                    if(row[get_key_answer(opt,i)]):
                        results[fact]['answers'].append(opt)
                        results[fact]['row_idx'].append(index)
                        results[fact]['fact_no'].append(i)
        
        for k in results:
            winner,should_repeat = get_winner(results[k]['answers'])
            results[k]['winner'] = winner
            results[k]['repeat'] = should_repeat
            if should_repeat:
                to_repeat_qids.append(results[k]['qid'])
        
    return results, to_repeat_qids

def write_results(results,output_file,analysis_str):
    results_df = pd.DataFrame.from_dict(results,orient='index')
    results_df = results_df.reset_index()
    results_df = results_df.drop(['row_idx','fact_no'],axis=1)
    with open('css_style.css','r') as css_file:
        CSS = css_file.read()
    with open(output_file,'w') as f:
        f.write(CSS+'\n\n')
        analysis_str = analysis_str.replace('\n','<br><br>')
        f.write(analysis_str+'\n\n')
        pd.set_option('display.max_colwidth', -1)
        results_df.to_html(f, escape=False, justify='center')


def read_result_files(args):
    dflist = []
    for result_file in args.result_file_list:
        df = pd.read_csv(result_file,engine='python')
        df = df[df['AssignmentStatus'] != 'Rejected']
        dflist.append(df)
    #
    return dflist



if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument('-rf_list', '--result_file_list', help="List of Names of the result csv(s) downloaded from mturk", required=True, nargs='+', default = [])
    parser.add_argument('-op', '--output_path', help="Output path for rejected people and results", required=True)
    parser.add_argument('-bf', '--book_file', help="Original HTML (Book) written by get_turk_data", required=True)
    parser.add_argument('-st', '--selection_type', help="How to select the winner? Through voting or majority?",type=str, default = 'voting')
    parser.add_argument('-thr', '--threshold', help="threhold for selecting winner.",type=float, default = 0)
    parser.add_argument('-mc', '--min_count', help="minimum count of valid answers to a particular question",type=int, default = 3)
    args = parser.parse_args()

    dflist = read_result_files(args)

    res_file_last_part_list = [os.path.basename(os.path.normpath(x)).split('.')[0] for x in args.result_file_list]


    os.makedirs(args.output_path, exist_ok=True)
    invalid_hits_list = [get_invalid_hits(df,os.path.join(args.output_path,res_file_last_part+'_rejected.csv')) for df,res_file_last_part in list(zip(dflist, res_file_last_part_list))]

    res_file_last_part = '_'.join(res_file_last_part_list)

    for invalid_hits in invalid_hits_list:
        if(len(invalid_hits)!=0):
            print('There are {} invalid assignments which have id \n{}'.format(len(list(itertools.chain(*list(invalid_hits.values())))),pprint.pformat(invalid_hits)))
            #exit(-1)

    book = get_book(args.book_file)
    results, qids_to_repeat = get_results(dflist, book)
    
    with open(os.path.join(args.output_path, res_file_last_part+'_repeat_qids_st{}_thr{}_mincount{}.csv'.format(args.selection_type, args.threshold, args.min_count)),'w') as fh:
        print('\n'.join(map(str,qids_to_repeat)),file = fh)
    #
    answers_list = []
    winner_list = []
    for k in results:
        answers_list.extend(results[k]['answers'])
        winner_list.extend(results[k]['winner'])

    ctr_answers = collections.Counter(answers_list)
    analysis_str = ''
    analysis_str += 'Total number of annotations = {}\n'.format(len(answers_list))
    for el in ctr_answers:
        ctr_answers[el] /= len(answers_list)*0.01
    analysis_str += '{}\n\n'.format(ctr_answers)

    ctr_winner = collections.Counter(winner_list)
    analysis_str += ('Total number of facts = {}\n'.format(len(results)))
    #analysis_str += ('Total number of winning facts = {}\n'.format(len(winner_list)-winner_list.count('tie')))
    analysis_str += ('Total number of winning facts = {}\n'.format(winner_list.count('other') + winner_list.count('our')))
    for el in ctr_winner:
        ctr_winner[el] /= len(winner_list)*0.01
    analysis_str += '{}\n\n'.format(ctr_winner)
    print(analysis_str)
    write_results(results,os.path.join(args.output_path,res_file_last_part+'_st{}_thr{}_analysis.html'.format(args.selection_type,args.threshold)),analysis_str)


